### SAC 하이퍼파라미터

1. **Batch Size**

   * **정의**: 한 번의 그라디언트 업데이트에 사용되는 샘플(transition) 개수입니다.
   * **영향**:

     * 너무 크면 메모리 사용량 및 계산 비용이 증가하고, 업데이트가 너무 안정적이어서 새로운 정보 반영이 느려집니다.
     * 너무 작으면 업데이트마다 분산(variance)이 커져 학습이 불안정해질 수 있습니다.
   * **권장 범위**: 64 – 256 (DeepRacer 콘솔 기본값은 256)
   * **튜닝 팁**: 처음에는 중간값(128 – 256)으로 시작하고, 메모리가 허용된다면 조금씩 키워 보며 학습 곡선 변화를 관찰하세요.

2. **Learning Rate**

   * **정의**: 네트워크 파라미터를 한 번 업데이트할 때 가중치 갱신 폭을 결정하는 값입니다.
   * **영향**:

     * 값이 크면 빠른 학습이 가능하지만 발산 위험이 커집니다.
     * 값이 작으면 안정적이지만 학습 속도가 느려집니다.
   * **권장 범위**: 3e-4 – 1e-3
   * **튜닝 팁**: 3e-4로 시작해, 발산 징후(보상 급등락)가 보이면 1e-4 수준으로 낮춰 보세요.

3. **SAC Alpha Value (온도 계수)**

   * **정의**: 정책의 엔트로피(무작위성)에 부여하는 가중치입니다.
   * **영향**:

     * 값이 크면 탐험을 더 오래 유지해 새로운 행동을 시도하지만, 수렴 속도가 느려집니다.
     * 값이 작으면 빠르게 수렴하지만 조기 수렴 위험이 있습니다.
   * **권장 범위**: 0.1 – 0.5 (자동 조정(automatic entropy tuning) 옵션 사용 가능)
   * **튜닝 팁**: 자동 조정을 켜면 학습 과정에서 α가 보상 규모에 맞춰 최적화됩니다. 수동 설정 시 0.2 – 0.3 사이를 먼저 시험해 보세요.

4. **Discount Factor (γ, Gamma)**

   * **정의**: 미래 보상에 대한 할인율로, 0 – 1 사이 값입니다.
   * **영향**:

     * γ가 높을수록 먼 미래 보상까지 고려해 더 장기적인 전략을 학습합니다.
     * γ가 낮으면 즉시 보상에 집중해 단기 성능이 올라가지만 최적 정책을 놓칠 수 있습니다.
   * **권장 범위**: 0.95 – 0.99
   * **튜닝 팁**: 트랙이 길고 보상이 멀리 분포되어 있으면 0.98 – 0.99, 짧은 트랙이면 0.95 – 0.97을 시도해 보세요.

5. **Loss Type**

   * **정의**: SAC는 크게 세 가지 손실을 사용합니다.

     1. **Policy Loss**: 최대 엔트로피 원칙에 따른 정책 손실
     2. **Q-Function Loss**: 벨만 방정식을 따르는 Q값 손실 (MSE)
     3. **Temperature Loss**: α를 자동 조정할 때 사용하는 손실
   * **DeepRacer 세팅**: 콘솔에서 직접 선택할 옵션은 없으며, 내부적으로 이 세 손실이 결합되어 최적화됩니다.

6. **Number of Experience Episodes Between Updates**

   * **정의**: 한 번 정책 또는 Q네트워크를 업데이트하기 전에 수집하는 에피소드(완주 혹은 조기 종료된 주행) 수입니다.
   * **영향**:

     * 너무 작으면 업데이트 빈도가 높아져 오버헤드가 커지고, 과도한 노이즈가 유입될 수 있습니다.
     * 너무 크면 업데이트 간격이 길어져 학습 반응 속도가 늦어집니다.
   * **권장 범위**: 8 – 32 에피소드
   * **튜닝 팁**: 트랙 한 바퀴 도는 데 시간이 오래 걸리면 에피소드 수를 줄이고, 짧으면 늘려서 샘플 효율을 맞춰 보세요.