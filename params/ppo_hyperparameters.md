
### PPO 하이퍼파라미터

1. **Batch Size**

   * 정책 업데이트 하나당 한 번에 처리하는 샘플(transition) 수입니다.
   * 너무 크면 메모리 사용량이 늘어나고, 너무 작으면 업데이트마다 분산(variance)이 커져 학습이 불안정해질 수 있습니다.
   * 일반적으로 64\~256 사이에서 실험하며, 샘플 효율성과 안정성의 균형을 맞춥니다.

2. **Epochs**

   * 한 번 수집된 배치 데이터를 사용하여 **몇 번** 학습(gradient descent)할지를 결정하는 값입니다.
   * Epochs가 크면 같은 데이터를 여러 번 학습하여 정책이 배치에 더 잘 맞춰지지만, 과적합(overfitting) 위험이 있습니다.
   * 보통 3\~10 사이로 설정하며, 권장 값은 5 전후입니다.

3. **Learning Rate**

   * 정책(policy)과 가치(value) 네트워크의 파라미터를 얼마나 크게 갱신할지를 결정하는 학습 속도입니다.
   * 값이 너무 크면 발산(divergence)이 발생할 수 있고, 너무 작으면 학습 속도가 매우 느려집니다.
   * DeepRacer 기본값은 대략 0.0003 정도이며, 0.0001 – 0.001 범위에서 조정해 보세요.

4. **Entropy Coefficient (Entropy Bonus)**

   * 정책의 무작위성(randomness)을 유지하기 위해, 손실 함수에 추가되는 엔트로피 항의 가중치입니다.
   * 값이 크면 탐험(exploration)을 오래 유지해 새로운 행동을 시도하지만, 너무 크면 최적의 정책 수렴이 더뎌집니다.
   * 보통 0.01 – 0.1 사이로 설정하며, PPO의 경우 기본 0.01 정도가 무난합니다.

5. **Discount Factor (γ, Gamma)**

   * 미래 보상(future reward)을 얼마나 중시할지를 결정하는 값입니다.
   * 0에 가까울수록 즉시 보상에만 집중하고, 1에 가까울수록 먼 미래의 보상까지 크게 고려합니다.
   * 대부분 연속 제어 작업에서는 0.95 – 0.99 사이를 사용합니다.

6. **Loss Type**

   * PPO의 경우 일반적으로 **클리핑(Clipped Surrogate) 손실**을 사용합니다.

     * `loss_type="clipped_surrogate"`: 정책 비율(policy ratio)을 1±ε 범위로 제한하여 과도한 업데이트를 방지
   * SAC의 경우 정책 손실(policy loss), Q함수 손실(Q-value loss), 온도(temperature) 손실을 조합합니다.

     * DeepRacer 콘솔에선 내부적으로 자동 설정되므로 보통 옵션을 직접 지정하지 않습니다.

7. **Experience Episodes Between Updates**

   * 한 번 정책을 업데이트하기 위해 몇 에피소드(완주 또는 종료된 주행)를 수집할지를 정합니다.
   * 예를 들어 16 에피소드로 설정하면, 16번 주행 데이터를 모은 뒤에 배치 크기만큼 샘플링하여 학습을 수행합니다.
   * 너무 작으면 업데이트 빈도가 높아져 오버헤드가 커지고, 너무 크면 업데이트 속도가 느립니다.
   * 보통 8 – 32 사이로 실험하여 트랙 길이와 에피소드 당 소요 시간에 맞춰 조절하세요.

---

**튜닝 팁**

* 먼저 `learning_rate`와 `batch_size`, `epochs` 조합을 찾아 안정적인 학습 곡선을 확보한 뒤,
* `entropy`와 `discount_factor`를 조정하여 탐험/수렴 속도를 최적화하시길 권장드립니다.
* 실제 트랙에서의 학습 안정성과 샘플 효율성은 환경(트랙 구성, 카메라 시점 등)에 크게 의존하므로, 작은 변화도 꼼꼼히 기록하며 테스트해 보세요.
